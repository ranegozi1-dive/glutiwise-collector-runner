name: MVP Bootstrap Stores + Prices (AUTO discover PRICE file types, ONE POST)

on:
  workflow_dispatch:

jobs:
  run:
    runs-on: ubuntu-latest
    steps:
      - name: Prepare folders
        run: |
          mkdir -p dumps

      - name: Pull scraper image
        run: |
          docker pull erlichsefi/israeli-supermarket-scarpers:latest

      # 1) Discover price-related ENABLED_FILE_TYPES from inside the image (no guessing)
      - name: Discover PRICE file type strings (from image)
        id: discover
        run: |
          set -e
          PRICE_TYPES="$(docker run --rm --entrypoint python3 erlichsefi/israeli-supermarket-scarpers:latest - <<'PY'
import re, sys, pkgutil, importlib, inspect

def find_price_tokens_in_module(mod):
    tokens = set()
    # Look for constants / lists / dicts that contain file type strings
    for name, val in vars(mod).items():
        if isinstance(val, (list, tuple, set)):
            for x in val:
                if isinstance(x, str) and re.search(r'price', x, re.I):
                    tokens.add(x)
        if isinstance(val, dict):
            for k, v in val.items():
                if isinstance(k, str) and re.search(r'price', k, re.I):
                    tokens.add(k)
                if isinstance(v, str) and re.search(r'price', v, re.I):
                    tokens.add(v)
        if isinstance(val, str) and re.search(r'price', val, re.I) and re.search(r'file', val, re.I):
            tokens.add(val)

    # Look for Enum-like classes with members containing 'PRICE'
    for name, obj in vars(mod).items():
        try:
            if inspect.isclass(obj) and hasattr(obj, "__members__"):
                for m in obj.__members__.values():
                    vv = getattr(m, "value", None)
                    if isinstance(vv, str) and re.search(r'price', vv, re.I):
                        tokens.add(vv)
                    if isinstance(m.name, str) and re.search(r'price', m.name, re.I):
                        tokens.add(m.name)
        except Exception:
            pass
    return tokens

def try_import_candidates():
    # Try a few likely module names first, then fall back to scanning package tree.
    candidates = [
        "il_supermarket_scraper.utils.file_types",
        "il_supermarket_scraper.file_types",
        "il_supermarket_scraper.utils",
        "il_supermarket_scraper",
    ]
    for c in candidates:
        try:
            yield importlib.import_module(c)
        except Exception:
            pass

def scan_all():
    # Scan top-level package modules for "file" + "type" keywords
    for m in pkgutil.iter_modules():
        name = m.name
        if "scraper" in name or "supermarket" in name:
            try:
                yield importlib.import_module(name)
            except Exception:
                pass

tokens = set()

for mod in try_import_candidates():
    tokens |= find_price_tokens_in_module(mod)

# If still empty, scan deeper inside the il_supermarket_scraper package if it exists
try:
    import il_supermarket_scraper
    pkg = il_supermarket_scraper
    for m in pkgutil.walk_packages(pkg.__path__, pkg.__name__ + "."):
        try:
            mod = importlib.import_module(m.name)
            tokens |= find_price_tokens_in_module(mod)
        except Exception:
            pass
except Exception:
    pass

# Hard stop if nothing found (no guessing)
tokens = sorted(tokens)
if not tokens:
    print("ERROR:NO_PRICE_FILE_TYPES_FOUND")
    sys.exit(2)

# Print as comma-separated for GitHub step output
print(",".join(tokens))
PY)"
          echo "Discovered PRICE types: $PRICE_TYPES"

          if echo "$PRICE_TYPES" | grep -q "ERROR:NO_PRICE_FILE_TYPES_FOUND"; then
            echo "Failed to discover PRICE file types from image. Aborting (no guessing)."
            exit 2
          fi

          echo "price_types=$PRICE_TYPES" >> "$GITHUB_OUTPUT"

      # 2) Download STORE + PRICE files in one docker run (minimize time/ops)
      - name: Download STORE + PRICE files (3 networks)
        env:
          PRICE_TYPES: ${{ steps.discover.outputs.price_types }}
        run: |
          set -e
          echo "Using PRICE file types: $PRICE_TYPES"
          docker run --rm \
            -v "${{ github.workspace }}/dumps:/usr/src/app/dumps" \
            -e ENABLED_SCRAPERS="RAMI_LEVY,SHUFERSAL,YOHANANOF" \
            -e ENABLED_FILE_TYPES="STORE_FILE,${PRICE_TYPES}" \
            -e LIMIT=120 \
            erlichsefi/israeli-supermarket-scarpers:latest

      # 3) Build ONE payload (stores up to 30 + records real) and POST to Make (ONE POST)
      - name: Build payload and POST to Make (ONE POST)
        env:
          MAKE_WEBHOOK_URL: ${{ secrets.MAKE_WEBHOOK_URL }}
          MAKE_APIKEY: ${{ secrets.MAKE_APIKEY }}
        run: |
          set -e
          python3 - << 'PY' | curl -sS -X POST "$MAKE_WEBHOOK_URL" \
              -H "Content-Type: application/json" \
              -H "x-make-apikey: $MAKE_APIKEY" \
              --data-binary @-
          import os, json, glob, re
          import xml.etree.ElementTree as ET
          from datetime import datetime, timezone

          # ====== HARD DEFAULTS (safe, no DB calls) ======
          # Use only barcodes that already exist in your DB (examples you gave)
          ALLOWLIST_BARCODES = {
              "7290000000011","7290000000028","7290000000035","7290000000042","7290000000059","7290000000066"
          }
          MAX_STORES_TOTAL = 30
          MAX_RECORDS_TOTAL = 200   # keep payload small
          MIN_RECORDS_TO_SEND = 2   # to satisfy your verification rule

          TARGET_TOKENS = [
              "הוד השרון", "כפר סבא", "רעננה", "הרצליה",
              "רמת השרון", "פתח תקווה", "תל אביב", "אחוזה"
          ]

          CHAINID_TO_RETAILER = {
              "7290027600007": "ramilevy",
              "7290058140886": "shufersal",
          }

          def norm(s: str) -> str:
              return re.sub(r"\s+", " ", (s or "").strip())

          def localname(tag: str) -> str:
              return tag.split("}")[-1] if "}" in tag else tag

          def looks_numeric(s: str) -> bool:
              return bool(re.fullmatch(r"\d{1,18}", (s or "").strip()))

          def find_first_text_anywhere(root, wanted_lower: str) -> str:
              for el in root.iter():
                  if localname(el.tag).lower() == wanted_lower and (el.text or "").strip():
                      return norm(el.text)
              return ""

          def text_in_subtree(elem, wanted_lower_set):
              for x in elem.iter():
                  if localname(x.tag).lower() in wanted_lower_set and (x.text or "").strip():
                      return norm(x.text)
              return ""

          def retailer_from_chain(chain_id: str, chain_name: str):
              if chain_id and chain_id in CHAINID_TO_RETAILER:
                  return CHAINID_TO_RETAILER[chain_id]
              cn = chain_name or ""
              if "שופרסל" in cn:
                  return "shufersal"
              if "רמי" in cn:
                  return "ramilevy"
              if "יוחננוף" in cn:
                  return "yohananof"
              return None

          def score(city, address, name):
              hay = norm(f"{city or ''} {address or ''} {name or ''}")
              hay2 = re.sub(r"[\s\-]", "", hay)
              for tok in TARGET_TOKENS:
                  if tok in hay:
                      return True
                  if re.sub(r"[\s\-]", "", tok) in hay2:
                      return True
              return False

          def city_from_shufersal_store_name(name: str) -> str:
              n = norm(name)
              if not n:
                  return ""
              parts = n.split(" ")
              if len(parts) >= 2:
                  last2 = " ".join(parts[-2:])
                  if re.search(r"[א-ת]", last2):
                      return last2
              last1 = parts[-1]
              if re.search(r"[א-ת]", last1):
                  return last1
              return ""

          def parse_store_file(path):
              root = ET.parse(path).getroot()
              chain_id = find_first_text_anywhere(root, "chainid")
              chain_name = find_first_text_anywhere(root, "chainname")
              stores = []
              for el in root.iter():
                  if localname(el.tag).lower() != "store":
                      continue
                  store_id = text_in_subtree(el, {"storeid"})
                  if not store_id:
                      continue
                  name = text_in_subtree(el, {"storename"}) or ""
                  address = text_in_subtree(el, {"address"}) or ""
                  if address.lower() == "unknown":
                      address = ""
                  city = text_in_subtree(el, {"city"}) or ""
                  if (not city) or city.lower() == "unknown" or looks_numeric(city):
                      city = ""
                  stores.append((store_id, city, address, name))
              return chain_id, chain_name, stores

          def make_store_obj(retailer, store_id, city, address, name):
              store_key = f"{retailer}:{store_id}"
              return {
                  "retailer": retailer,
                  "store_id": str(store_id),
                  "store_key": store_key,
                  "store_name": name or "",
                  "city": city or "",
                  "address": address or "",
                  "area": "",
                  "store_type": ""
              }

          # ====== Select stores up to 30 (existing logic, slightly compact) ======
          candidates = {"ramilevy": [], "shufersal": [], "yohananof": []}
          for path in glob.glob("dumps/**/*.xml", recursive=True):
              base = os.path.basename(path).lower()
              if not base.startswith("stores"):
                  continue
              chain_id, chain_name, stores = parse_store_file(path)
              retailer = retailer_from_chain(chain_id, chain_name)
              if retailer in candidates and stores:
                  candidates[retailer].extend(stores)

          selected_stores = []
          seen = set()
          def add_store(retailer, store_id, city, address, name):
              if retailer == "shufersal" and (not city or looks_numeric(city) or city.lower() == "unknown"):
                  city = city_from_shufersal_store_name(name)
              obj = make_store_obj(retailer, store_id, city, address, name)
              sk = obj["store_key"]
              if sk in seen:
                  return
              seen.add(sk)
              selected_stores.append(obj)

          # Prefer target-area stores first, then fill
          for retailer in ("ramilevy","shufersal","yohananof"):
              preferred, fallback = [], []
              for store_id, city, address, name in candidates.get(retailer, []):
                  (preferred if score(city,address,name) else fallback).append((store_id, city, address, name))
              for tup in preferred + fallback:
                  add_store(retailer, *tup)
                  if len(selected_stores) >= MAX_STORES_TOTAL:
                      break
              if len(selected_stores) >= MAX_STORES_TOTAL:
                  break

          if not selected_stores:
              raise SystemExit("No stores selected from STORE files; aborting.")

          store_keys = [s["store_key"] for s in selected_stores]

          # ====== PRICE parsing (generic + safe) ======
          PRICE_TAGS_BARCODE = {"itemcode","itemid","code","barcode","gtin"}
          PRICE_TAGS_PRICE   = {"itemprice","price","unitprice","saleprice","priceafterdiscount"}

          DATE_TAGS = {"pricedate","lastupdatedate","updatedate","priceupdatedate","updateDate","lastUpdateDate"}

          def safe_float(x):
              x = (x or "").strip().replace(",", ".")
              try:
                  v = float(x)
                  if v < 0:
                      return None
                  return v
              except Exception:
                  return None

          def extract_any(root, tagset):
              for el in root.iter():
                  t = localname(el.tag).lower()
                  if t in tagset and (el.text or "").strip():
                      return norm(el.text)
              return ""

          def iter_item_like_nodes(root):
              # Try to find repeated "Item" nodes; if none, fall back to whole tree
              items = []
              for el in root.iter():
                  if localname(el.tag).lower() in {"item","product"}:
                      items.append(el)
              return items if items else [root]

          def parse_price_file(path):
              root = ET.parse(path).getroot()
              chain_id = extract_any(root, {"chainid"})
              chain_name = extract_any(root, {"chainname"})
              store_id = extract_any(root, {"storeid"})
              retailer = retailer_from_chain(chain_id, chain_name)
              if not retailer or not store_id:
                  return []

              store_key = f"{retailer}:{store_id}"
              # Only keep prices for the stores we actually loaded (MVP constraint)
              if store_key not in set(store_keys):
                  return []

              updated_at = extract_any(root, {t.lower() for t in DATE_TAGS})
              # If date missing/invalid -> use now (safe default)
              now = datetime.now(timezone.utc).replace(microsecond=0).isoformat().replace("+00:00","Z")
              if not updated_at:
                  updated_at = now

              out = []
              for node in iter_item_like_nodes(root):
                  bc = extract_any(node, PRICE_TAGS_BARCODE)
                  pr = extract_any(node, PRICE_TAGS_PRICE)

                  bc = (bc or "").strip()
                  if bc not in ALLOWLIST_BARCODES:
                      continue

                  price_val = safe_float(pr)
                  if price_val is None:
                      continue

                  price_key = f"{bc}:{store_key}"
                  out.append({
                      "price_key": price_key,
                      "barcode": bc,
                      "store_key": store_key,
                      "price": price_val,
                      "updated_at": updated_at
                  })
              return out

          # Identify "non-store" XMLs as price candidates (we don't guess filenames beyond excluding stores*)
          price_records_raw = []
          for path in glob.glob("dumps/**/*.xml", recursive=True):
              base = os.path.basename(path).lower()
              if base.startswith("stores"):
                  continue
              try:
                  recs = parse_price_file(path)
                  if recs:
                      price_records_raw.extend(recs)
              except Exception:
                  # If a file isn't parseable as XML, ignore (safe)
                  continue

          # Dedupe latest-only inside this run by price_key (keep last occurrence)
          dedup = {}
          for r in price_records_raw:
              dedup[r["price_key"]] = r

          records = list(dedup.values())
          # Keep payload bounded
          records = records[:MAX_RECORDS_TOTAL]

          generated_at = datetime.now(timezone.utc).replace(microsecond=0).isoformat().replace("+00:00","Z")

          payload = {
              "source": "collector_prices_3networks_autodiscovery",
              "generated_at": generated_at,
              "stores": selected_stores,
              "records": records
          }

          # If we didn't manage to extract enough records, still POST (so you can see import_errors),
          # but we also add a clear marker in source.
          if len(records) < MIN_RECORDS_TO_SEND:
              payload["source"] = "collector_prices_3networks_autodiscovery__LOW_RECORDS"

          print(json.dumps(payload, ensure_ascii=False))
          PY
